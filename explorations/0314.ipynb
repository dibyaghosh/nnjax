{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring PJIT today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAX_PLATFORMS'] = 'cpu'\n",
    "import jax\n",
    "jax.config.update('jax_num_cpu_devices', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import sharding as shd\n",
    "from jax._src.mesh import get_concrete_mesh\n",
    "\n",
    "def P(*args):\n",
    "    return shd.NamedSharding(get_concrete_mesh(), shd.PartitionSpec(*args))\n",
    "\n",
    "def see_compiled(fn, *args):\n",
    "    compiled = fn.lower(*args).compile()\n",
    "    print(compiled.as_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.shard import reshard\n",
    "\n",
    "bs = 16\n",
    "d = 32\n",
    "m = 128\n",
    "\n",
    "shd.set_mesh(jax.make_mesh(axis_shapes=(8,), axis_names=('X',), axis_types=(shd.AxisType.Explicit,)))\n",
    "\n",
    "X = jnp.zeros((bs, d), device=P())\n",
    "W1 = jnp.zeros((d, m), device=P())\n",
    "W2 = jnp.zeros((m, d), device=P())\n",
    "\n",
    "def f(X, W1, W2):\n",
    "    return jax.nn.relu(X @ W1) @ W2\n",
    "# see_compiled(jax.jit(f), X, W1, W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ENTRY %main.18_spmd (param: f32[16,32], param.1: f32[32,128], param.2: f32[128,32]) -> f32[16,32] {\n",
    "  # Parameters\n",
    "  %param = f32[16,32]{1,0} parameter(0), sharding={replicated}, metadata={op_name=\"X\"}\n",
    "  %param.1 = f32[32,128]{1,0} parameter(1), sharding={replicated}, metadata={op_name=\"W1\"}\n",
    "  %param.2 = f32[128,32]{1,0} parameter(2), sharding={replicated}, metadata={op_name=\"W2\"}\n",
    "  # First matmul\n",
    "  %dot = f32[16,128]{1,0} dot(f32[16,32]{1,0} %param, f32[32,128]{1,0} %param.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name=\"jit(f)/jit(main)/dot_general\" source_file=\"/tmp/ipykernel_234850/1341430039.py\" source_line=14}\n",
    "  # ReLU\n",
    "  %broadcast_maximum_fusion = f32[16,128]{1,0} fusion(f32[16,128]{1,0} %dot), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(f)/jit(main)/jit(relu)/max\" source_file=\"/tmp/ipykernel_234850/1341430039.py\" source_line=14}\n",
    "  # Second matmul\n",
    "  ROOT %dot.1 = f32[16,32]{1,0} dot(f32[16,128]{1,0} %broadcast_maximum_fusion, f32[128,32]{1,0} %param.2), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name=\"jit(f)/jit(main)/dot_general\" source_file=\"/tmp/ipykernel_234850/1341430039.py\" source_line=14}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HloModule jit_f, is_scheduled=true, entry_computation_layout={(f32[2,32]{1,0}, f32[32,128]{1,0}, f32[128,32]{1,0})->f32[2,32]{1,0}}, allow_spmd_sharding_propagation_to_parameters={false,false,false}, num_partitions=8\n",
      "\n",
      "%fused_computation (param_0: f32[2,128]) -> f32[2,128] {\n",
      "  %param_0 = f32[2,128]{1,0} parameter(0)\n",
      "  %constant.2 = f32[] constant(0)\n",
      "  %broadcast.2 = f32[2,128]{1,0} broadcast(f32[] %constant.2), dimensions={}, metadata={op_name=\"jit(f)/jit(main)/jit(relu)/max\" source_file=\"/tmp/ipykernel_234850/1296888503.py\" source_line=6}\n",
      "  ROOT %maximum.2 = f32[2,128]{1,0} maximum(f32[2,128]{1,0} %param_0, f32[2,128]{1,0} %broadcast.2), metadata={op_name=\"jit(f)/jit(main)/jit(relu)/max\" source_file=\"/tmp/ipykernel_234850/1296888503.py\" source_line=6}\n",
      "}\n",
      "\n",
      "ENTRY %main.18_spmd (param: f32[2,32], param.1: f32[32,128], param.2: f32[128,32]) -> f32[2,32] {\n",
      "  %param = f32[2,32]{1,0} parameter(0), sharding={devices=[8,1]<=[8]}, metadata={op_name=\"X\"}\n",
      "  %param.1 = f32[32,128]{1,0} parameter(1), sharding={replicated}, metadata={op_name=\"W1\"}\n",
      "  %param.2 = f32[128,32]{1,0} parameter(2), sharding={replicated}, metadata={op_name=\"W2\"}\n",
      "  %dot = f32[2,128]{1,0} dot(f32[2,32]{1,0} %param, f32[32,128]{1,0} %param.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name=\"jit(f)/jit(main)/dot_general\" source_file=\"/tmp/ipykernel_234850/1296888503.py\" source_line=6}\n",
      "  %broadcast_maximum_fusion = f32[2,128]{1,0} fusion(f32[2,128]{1,0} %dot), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(f)/jit(main)/jit(relu)/max\" source_file=\"/tmp/ipykernel_234850/1296888503.py\" source_line=6}\n",
      "  ROOT %dot.1 = f32[2,32]{1,0} dot(f32[2,128]{1,0} %broadcast_maximum_fusion, f32[128,32]{1,0} %param.2), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name=\"jit(f)/jit(main)/dot_general\" source_file=\"/tmp/ipykernel_234850/1296888503.py\" source_line=6}\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = jnp.zeros((bs, d), device=P('X'))\n",
    "W1 = jnp.zeros((d, m), device=P())\n",
    "W2 = jnp.zeros((m, d), device=P())\n",
    "\n",
    "def f(X, W1, W2):\n",
    "    return jax.nn.relu(X @ W1) @ W2\n",
    "see_compiled(jax.jit(f), X, W1, W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HloModule jit_f, is_scheduled=true, entry_computation_layout={(f32[2,32]{1,0}, f32[32,16]{1,0}, f32[16,32]{1,0})->f32[2,32]{1,0}}, allow_spmd_sharding_propagation_to_parameters={false,false,false}, num_partitions=8\n",
      "\n",
      "%fused_computation (param_0: f32[2,128]) -> f32[2,128] {\n",
      "  %param_0 = f32[2,128]{1,0} parameter(0)\n",
      "  %constant.2 = f32[] constant(0)\n",
      "  %broadcast.2 = f32[2,128]{1,0} broadcast(f32[] %constant.2), dimensions={}, metadata={op_name=\"jit(f)/jit(main)/jit(relu)/max\" source_file=\"/tmp/ipykernel_234850/1296888503.py\" source_line=6}\n",
      "  ROOT %maximum.2 = f32[2,128]{1,0} maximum(f32[2,128]{1,0} %param_0, f32[2,128]{1,0} %broadcast.2), metadata={op_name=\"jit(f)/jit(main)/jit(relu)/max\" source_file=\"/tmp/ipykernel_234850/1296888503.py\" source_line=6}\n",
      "}\n",
      "\n",
      "ENTRY %main.20_spmd (param: f32[2,32], param.1: f32[32,16], param.2: f32[16,32]) -> f32[2,32] {\n",
      "  %param = f32[2,32]{1,0} parameter(0), sharding={devices=[8,1]<=[8]}, metadata={op_name=\"X\"}\n",
      "  %param.1 = f32[32,16]{1,0} parameter(1), sharding={devices=[1,8]<=[8]}, metadata={op_name=\"W1\"}\n",
      "  %param.2 = f32[16,32]{1,0} parameter(2), sharding={devices=[8,1]<=[8]}, metadata={op_name=\"W2\"}\n",
      "  %copy.12 = f32[32,16]{0,1} copy(f32[32,16]{1,0} %param.1), sharding={devices=[1,8]<=[8]}, metadata={op_name=\"W1\"}\n",
      "  %all-gather.1 = f32[128,32]{1,0} all-gather(f32[16,32]{1,0} %param.2), channel_id=2, replica_groups=[1,8]<=[8], dimensions={0}, use_global_device_ids=true, metadata={op_name=\"jit(f)/jit(main)/reshard\" source_file=\"/tmp/ipykernel_234850/1889149736.py\" source_line=6}\n",
      "  %all-gather = f32[32,128]{0,1} all-gather(f32[32,16]{0,1} %copy.12), channel_id=1, replica_groups=[1,8]<=[8], dimensions={1}, use_global_device_ids=true, metadata={op_name=\"jit(f)/jit(main)/reshard\" source_file=\"/tmp/ipykernel_234850/1889149736.py\" source_line=6}\n",
      "  %copy.13 = f32[32,128]{1,0} copy(f32[32,128]{0,1} %all-gather), metadata={op_name=\"jit(f)/jit(main)/reshard\" source_file=\"/tmp/ipykernel_234850/1889149736.py\" source_line=6}\n",
      "  %dot = f32[2,128]{1,0} dot(f32[2,32]{1,0} %param, f32[32,128]{1,0} %copy.13), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name=\"jit(f)/jit(main)/dot_general\" source_file=\"/tmp/ipykernel_234850/1889149736.py\" source_line=6}\n",
      "  %broadcast_maximum_fusion = f32[2,128]{1,0} fusion(f32[2,128]{1,0} %dot), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(f)/jit(main)/jit(relu)/max\" source_file=\"/tmp/ipykernel_234850/1296888503.py\" source_line=6}\n",
      "  ROOT %dot.1 = f32[2,32]{1,0} dot(f32[2,128]{1,0} %broadcast_maximum_fusion, f32[128,32]{1,0} %all-gather.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name=\"jit(f)/jit(main)/dot_general\" source_file=\"/tmp/ipykernel_234850/1889149736.py\" source_line=6}\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = jnp.zeros((bs, d), device=P('X'))\n",
    "W1 = jnp.zeros((d, m), device=P(None, 'X'))\n",
    "W2 = jnp.zeros((m, d), device=P('X'))\n",
    "\n",
    "def f(X, W1, W2):\n",
    "    return jax.nn.relu(X @ reshard(W1, P())) @ reshard(W2, P())\n",
    "\n",
    "see_compiled(jax.jit(f), X, W1, W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Parallel (Model Parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.typeof(mid)=ShapedArray(float32[16,128@X]) jax.typeof(W2)=ShapedArray(float32[128@X,32])\n",
      "jax.typeof(out)=ShapedArray(float32[16,32])\n",
      "HloModule jit_f, is_scheduled=true, entry_computation_layout={(f32[16,32]{1,0}, f32[32,16]{1,0}, f32[16,32]{1,0})->f32[16,32]{1,0}}, allow_spmd_sharding_propagation_to_parameters={false,false,false}, num_partitions=8\n",
      "\n",
      "%add.clone (x.1: f32[], y.1: f32[]) -> f32[] {\n",
      "  %x.1 = f32[] parameter(0)\n",
      "  %y.1 = f32[] parameter(1)\n",
      "  ROOT %add.1 = f32[] add(f32[] %x.1, f32[] %y.1)\n",
      "}\n",
      "\n",
      "%fused_computation (param_0: f32[16,16]) -> f32[16,16] {\n",
      "  %param_0 = f32[16,16]{1,0} parameter(0)\n",
      "  %constant.4 = f32[] constant(0)\n",
      "  %broadcast.2 = f32[16,16]{1,0} broadcast(f32[] %constant.4), dimensions={}, metadata={op_name=\"jit(f)/jit(main)/jit(relu)/max\" source_file=\"/tmp/ipykernel_234850/3672491555.py\" source_line=6}\n",
      "  ROOT %maximum.2 = f32[16,16]{1,0} maximum(f32[16,16]{1,0} %param_0, f32[16,16]{1,0} %broadcast.2), metadata={op_name=\"jit(f)/jit(main)/jit(relu)/max\" source_file=\"/tmp/ipykernel_234850/3672491555.py\" source_line=6}\n",
      "}\n",
      "\n",
      "ENTRY %main.18_spmd (param: f32[16,32], param.1: f32[32,16], param.2: f32[16,32]) -> f32[16,32] {\n",
      "  %param = f32[16,32]{1,0} parameter(0), sharding={replicated}, metadata={op_name=\"X\"}\n",
      "  %param.1 = f32[32,16]{1,0} parameter(1), sharding={devices=[1,8]<=[8]}, metadata={op_name=\"W1\"}\n",
      "  %param.2 = f32[16,32]{1,0} parameter(2), sharding={devices=[8,1]<=[8]}, metadata={op_name=\"W2\"}\n",
      "  %dot = f32[16,16]{1,0} dot(f32[16,32]{1,0} %param, f32[32,16]{1,0} %param.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name=\"jit(f)/jit(main)/dot_general\" source_file=\"/tmp/ipykernel_234850/2016343584.py\" source_line=6}\n",
      "  %broadcast_maximum_fusion = f32[16,16]{1,0} fusion(f32[16,16]{1,0} %dot), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(f)/jit(main)/jit(relu)/max\" source_file=\"/tmp/ipykernel_234850/3672491555.py\" source_line=6}\n",
      "  %dot.1 = f32[16,32]{1,0} dot(f32[16,16]{1,0} %broadcast_maximum_fusion, f32[16,32]{1,0} %param.2), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name=\"jit(f)/jit(main)/bm,md->bd/dot_general\" source_file=\"/tmp/ipykernel_234850/2016343584.py\" source_line=8}\n",
      "  ROOT %all-reduce = f32[16,32]{1,0} all-reduce(f32[16,32]{1,0} %dot.1), channel_id=1, replica_groups=[1,8]<=[8], use_global_device_ids=true, to_apply=%add.clone, metadata={op_name=\"jit(f)/jit(main)/bm,md->bd/dot_general\" source_file=\"/tmp/ipykernel_234850/2016343584.py\" source_line=8}\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = jnp.zeros((bs, d), device=P())\n",
    "W1 = jnp.zeros((d, m), device=P(None, 'X'))\n",
    "W2 = jnp.zeros((m, d), device=P('X'))\n",
    "\n",
    "def f(X, W1, W2):\n",
    "    mid = jax.nn.relu(X @ W1)\n",
    "    print(f\"{jax.typeof(mid)=} {jax.typeof(W2)=}\")\n",
    "    out = jnp.einsum('bm,md->bd', mid, W2, out_sharding=P())\n",
    "    print(f\"{jax.typeof(out)=}\")\n",
    "    return out\n",
    "see_compiled(jax.jit(f), X, W1, W2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
